import streamlit as st
import sqlite3
import datetime
import pandas as pd
import plotly.graph_objects as go
from PIL import Image, ImageDraw, ImageFont 
import io 
from pathlib import Path 

# --- í˜ì´ì§€ ì„¤ì • (ë°˜ë“œì‹œ ì²« ë²ˆì§¸ st ëª…ë ¹ì–´ì•¼ í•¨) ---
st.set_page_config(page_title="ì¤‘ê³  ì•„ì´í° ë¶„ì„", layout="wide")

# --- ìŠ¤í¬ë¦½íŠ¸ì˜ í˜„ì¬ ìœ„ì¹˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ 'CSV' íŒŒì¼ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ì§€ì • ---
BASE_DIR = Path(__file__).parent 
MAP_FILE_PATH = BASE_DIR / "dong_gu_map.csv" # ğŸ‘ˆ 'dong_gu_map.csv'ë¥¼ ì°¾ë„ë¡ ìˆ˜ì •

# --- 1. ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ ê²½ë¡œ (ì‚¬ìš©ì ì ˆëŒ€ ê²½ë¡œ) ---
DB_FILE = r"C:\Users\jjunh\OneDrive\ë°”íƒ• í™”ë©´\ëŒ€í•™\4í•™ë…„ 2í•™ê¸°\ë°ì´í„°ë² ì´ìŠ¤\ë°.ë²  êµ¬ì¶•\project2.db"

# --- [ì§€ë„] ì„œìš¸ 25ê°œ êµ¬ í”½ì…€ ì¢Œí‘œ ë§µ (x, y) ---
SEOUL_GU_COORDINATES = {
    'ê°•ë‚¨êµ¬': (335, 260), 'ê°•ë™êµ¬': (410, 240), 'ê°•ë¶êµ¬': (275, 90),
    'ê°•ì„œêµ¬': (40, 200), 'ê´€ì•…êµ¬': (210, 310), 'ê´‘ì§„êµ¬': (350, 215),
    'êµ¬ë¡œêµ¬': (80, 280), 'ê¸ˆì²œêµ¬': (130, 320), 'ë…¸ì›êµ¬': (370, 70),
    'ë„ë´‰êµ¬': (320, 55), 'ë™ëŒ€ë¬¸êµ¬': (310, 155), 'ë™ì‘êµ¬': (220, 265),
    'ë§ˆí¬êµ¬': (140, 190), 'ì„œëŒ€ë¬¸êµ¬': (170, 160), 'ì„œì´ˆêµ¬': (280, 280),
    'ì„±ë™êµ¬': (300, 210), 'ì„±ë¶êµ¬': (270, 125), 'ì†¡íŒŒêµ¬': (380, 260),
    'ì–‘ì²œêµ¬': (75, 240), 'ì˜ë“±í¬êµ¬': (150, 250), 'ìš©ì‚°êµ¬': (250, 230),
    'ì€í‰êµ¬': (140, 100), 'ì¢…ë¡œêµ¬': (240, 160), 'ì¤‘êµ¬': (250, 190),
    'ì¤‘ë‘êµ¬': (360, 130)
}

# --- DB ì—°ê²° ---
def get_db_connection():
    """ë°ì´í„°ë² ì´ìŠ¤ì— ì—°ê²°í•˜ê³  connection ê°ì²´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        conn = sqlite3.connect(f"file:{DB_FILE}?mode=ro", uri=True) 
        return conn
    except Exception as e:
        st.error(f"DB ì—°ê²° ì˜¤ë¥˜: {e}")
        st.error(f"ì§€ì •ëœ DB íŒŒì¼ ê²½ë¡œ: {DB_FILE}")
        return None

# --- CSV íŒŒì¼ì„ ì½ë„ë¡ pd.read_csvë¡œ ìˆ˜ì • ---
@st.cache_data
def load_manual_map_sql_chunk(file_path): 
    """
    'dong_gu_map.csv' CSV íŒŒì¼ì„ ì½ì–´ SQLì˜ 'UNION SELECT ...' êµ¬ë¬¸ ë©ì–´ë¦¬ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    try:
        df = pd.read_csv(file_path, encoding='utf-8') 
        
        if 'dong' not in df.columns or 'sigungu' not in df.columns:
            st.error(f"'{file_path}'ì— 'dong' ë˜ëŠ” 'sigungu' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return ""
            
        union_statements = []
        for index, row in df.iterrows():
            dong_clean = str(row['dong']).replace("'", "''")
            sigungu_clean = str(row['sigungu']).replace("'", "''")
            
            if dong_clean and sigungu_clean: 
                union_statements.append(f"SELECT '{dong_clean}', '{sigungu_clean}'")
        
        if union_statements:
            return " UNION ".join(union_statements)
        else:
            return ""
            
    except FileNotFoundError:
        st.warning(f"ìˆ˜ë™ ë§¤í•‘ íŒŒì¼('{file_path}')ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. DB ë°ì´í„°ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return ""
    except Exception as e:
        st.error(f"CSV ë§¤í•‘ íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜ ({file_path}): {e}") 
        return ""

# --- í—¬í¼ í•¨ìˆ˜: ë™ì  SQL ì¿¼ë¦¬ ìƒì„± ---
def build_dynamic_query_parts(platform, model, start_date, end_date):
    """
    í•„í„° ì¡°ê±´ì— ë”°ë¼ SQL WHEREì ˆê³¼ íŒŒë¼ë¯¸í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    params = []
    where_clause = " WHERE p.posted_date BETWEEN ? AND ? "
    params.extend([str(start_date), str(end_date)])

    if platform != 'ì „ì²´':
        where_clause += " AND pf.name = ? "
        params.append(platform)

    if model == 'iPhone 16 Pro':
        where_clause += " AND (pr.model LIKE ? OR pr.model LIKE ?) "
        params.extend(['%iPhone 16 Pro%', '%ì•„ì´í° 16 í”„ë¡œ%'])
    elif model == 'iPhone 16':
        where_clause += " AND (pr.model LIKE ? OR pr.model LIKE ?) "
        params.extend(['%iPhone 16%', '%ì•„ì´í° 16%'])
        where_clause += " AND pr.model NOT LIKE '%Pro%' AND pr.model NOT LIKE '%í”„ë¡œ%' "
    elif model == 'iPhone 15 Pro':
        where_clause += " AND (pr.model LIKE ? OR pr.model LIKE ?) "
        params.extend(['%iPhone 15 Pro%', '%ì•„ì´í° 15 í”„ë¡œ%'])
    elif model == 'iPhone 15': 
        where_clause += " AND (pr.model LIKE ? OR pr.model LIKE ?) "
        params.extend(['%iPhone 15%', '%ì•„ì´í° 15%'])
        where_clause += " AND pr.model NOT LIKE '%Pro%' AND pr.model NOT LIKE '%í”„ë¡œ%' "
    elif model == 'iPhone 14 Pro':
        where_clause += " AND (pr.model LIKE ? OR pr.model LIKE ?) "
        params.extend(['%iPhone 14 Pro%', '%ì•„ì´í° 14 í”„ë¡œ%'])
    elif model == 'iPhone 14': 
        where_clause += " AND (pr.model LIKE ? OR pr.model LIKE ?) "
        params.extend(['%iPhone 14%', '%ì•„ì´í° 14%'])
        where_clause += " AND pr.model NOT LIKE '%Pro%' AND pr.model NOT LIKE '%í”„ë¡œ%' "
        
    return where_clause, params

# --- ë°±ì—”ë“œ í•¨ìˆ˜ 1: KPI ë° ID ---
@st.cache_data
def fetch_kpi_and_ids(platform, model, start_date, end_date):
    """'ì´ ë§¤ë¬¼ ìˆ˜', 'í‰ê·  ê°€ê²©', 'ê²Œì‹œê¸€ ID ë¦¬ìŠ¤íŠ¸'ë¥¼ DBì—ì„œ ì¡°íšŒ"""
    
    where_clause, params = build_dynamic_query_parts(platform, model, start_date, end_date)
    sql = f"""
    SELECT
        COUNT(p.post_id), AVG(p.price_krw), GROUP_CONCAT(p.post_id, ', ')
    FROM posts AS p
    JOIN platforms AS pf ON p.platform_id = pf.platform_id
    JOIN products AS pr ON p.product_id = pr.product_id
    {where_clause}
    """
    
    conn = get_db_connection()
    if conn is None: return 0, 0, []

    total_count, avg_price, id_list_str = 0, 0, ""
    try:
        cursor = conn.cursor()
        cursor.execute(sql, params)
        result = cursor.fetchone()
        if result:
            total_count = result[0] if result[0] is not None else 0
            avg_price = result[1] if result[1] is not None else 0
            id_list_str = result[2] if result[2] is not None else ""
    except Exception as e:
        st.error(f"KPI ì¿¼ë¦¬ ì˜¤ë¥˜: {e}")
    finally:
        if conn:
            conn.close() 

    id_list = id_list_str.split(', ') if id_list_str else []
    return total_count, avg_price, id_list

# --- ğŸ’¡ğŸ’¡ğŸ’¡ [ì‹ ê·œ ì¶”ê°€] ë§¤í•‘ì— ì‹¤íŒ¨í•œ 'ë™' ëª©ë¡ì„ ê°€ì ¸ì˜¤ëŠ” ë””ë²„ê·¸ í•¨ìˆ˜ ğŸ’¡ğŸ’¡ğŸ’¡ ---
@st.cache_data
def fetch_unmapped_dongs(platform, model, start_date, end_date, map_file_path):
    """ë§¤í•‘ì´ í•„ìš”í•œ(sigungu=NULL) ë°ì´í„° ì¤‘, CSV ë§µì—ì„œ ë§¤í•‘ì— ì‹¤íŒ¨í•œ 'ë™' ëª©ë¡ ì¡°íšŒ"""
    
    where_clause, params = build_dynamic_query_parts(platform, model, start_date, end_date)
    
    # ë™ì¼í•œ ë¡œì§ìœ¼ë¡œ CSV ë§µ ë¡œë“œ
    manual_map_sql_chunk = load_manual_map_sql_chunk(map_file_path)

    sql = f"""
    WITH DongGuMapSource AS (
        SELECT dong, sigungu 
        FROM regions
        WHERE sigungu IS NOT NULL AND sigungu != '' AND dong IS NOT NULL AND dong != ''
        {"UNION " + manual_map_sql_chunk if manual_map_sql_chunk else ""} 
    ),
    DongGuMap AS (
        SELECT 
            REPLACE(TRIM(dong), ' ', '') AS clean_dong, 
            MAX(sigungu) AS mapped_sigungu
        FROM DongGuMapSource
        WHERE dong IS NOT NULL AND sigungu IS NOT NULL
        GROUP BY clean_dong
    )
    -- [ìˆ˜ì •] ë§¤í•‘ì— 'ì‹¤íŒ¨'í•œ ëª©ë¡ì„ ì°¾ëŠ” ì¿¼ë¦¬
    SELECT 
        r.dong AS "DBì— ì €ì¥ëœ 'ë™' ì´ë¦„", 
        COUNT(p.post_id) as "ë§¤ë¬¼ ìˆ˜"
    FROM posts AS p
    JOIN platforms AS pf ON p.platform_id = pf.platform_id
    JOIN products AS pr ON p.product_id = pr.product_id
    JOIN regions AS r ON p.region_id = r.region_id
    LEFT JOIN DongGuMap AS dgm 
        ON REPLACE(TRIM(r.dong), ' ', '') = dgm.clean_dong
    {where_clause} 
    AND r.sigungu IS NULL -- 1. 'êµ¬'ê°€ NULLì´ë¼ ë§¤í•‘ì´ í•„ìš”í•œë°
    AND dgm.mapped_sigungu IS NULL -- 2. ë§µ(DongGuMap)ì—ì„œ ì¼ì¹˜í•˜ëŠ” 'ë™'ì„ ëª» ì°¾ì•˜ê³ 
    AND r.dong IS NOT NULL AND r.dong != '' -- 3. 'ë™' ì´ë¦„ ìì²´ëŠ” ì¡´ì¬í•˜ëŠ” ê²½ìš°
    GROUP BY r.dong
    ORDER BY "ë§¤ë¬¼ ìˆ˜" DESC
    """
    
    conn = get_db_connection()
    if conn is None: return pd.DataFrame(columns=["DBì— ì €ì¥ëœ 'ë™' ì´ë¦„", "ë§¤ë¬¼ ìˆ˜"])
    
    try:
        df = pd.read_sql_query(sql, conn, params=params)
        return df
    except Exception as e:
        st.error(f"ë§¤í•‘ ì‹¤íŒ¨ ì¿¼ë¦¬ ì˜¤ë¥˜: {e}")
        return pd.DataFrame(columns=["DBì— ì €ì¥ëœ 'ë™' ì´ë¦„", "ë§¤ë¬¼ ìˆ˜"])
    finally:
        if conn:
            conn.close() 

# --- 'dong' ì´ë¦„ì˜ ê³µë°±ì„ ì œê±°(TRIM, REPLACE)í•˜ëŠ” SQLë¡œ ìˆ˜ì •ëœ í•¨ìˆ˜ ---
@st.cache_data
def fetch_regional_data(platform, model, start_date, end_date, map_file_path): # ğŸ‘ˆ ì¸ì ì¶”ê°€
    """ì§€ì—­ë³„(sigungu) ë§¤ë¬¼ ë¶„í¬ Top 10 ì¡°íšŒ (CSV íŒŒì¼ ê¸°ë°˜ + DB ë°ì´í„°ë¡œ ë³´ì™„)"""
    
    where_clause, params = build_dynamic_query_parts(platform, model, start_date, end_date)
    
    manual_map_sql_chunk = load_manual_map_sql_chunk(map_file_path) 

    sql = f"""
    WITH DongGuMapSource AS (
        SELECT dong, sigungu 
        FROM regions
        WHERE sigungu IS NOT NULL AND sigungu != ''
          AND dong IS NOT NULL AND dong != ''
        {"UNION " + manual_map_sql_chunk if manual_map_sql_chunk else ""} 
    ),
    DongGuMap AS (
        SELECT 
            REPLACE(TRIM(dong), ' ', '') AS clean_dong, 
            MAX(sigungu) AS mapped_sigungu
        FROM DongGuMapSource
        WHERE dong IS NOT NULL AND sigungu IS NOT NULL
        GROUP BY clean_dong
    )
    SELECT 
        COALESCE(r.sigungu, dgm.mapped_sigungu) AS final_gu,
        COUNT(p.post_id) as count
    FROM posts AS p
    JOIN platforms AS pf ON p.platform_id = pf.platform_id
    JOIN products AS pr ON p.product_id = pr.product_id
    JOIN regions AS r ON p.region_id = r.region_id
    LEFT JOIN DongGuMap AS dgm 
    ON REPLACE(TRIM(r.dong), ' ', '') = dgm.clean_dong
    {where_clause}
    AND COALESCE(r.sigungu, dgm.mapped_sigungu) IS NOT NULL 
    AND COALESCE(r.sigungu, dgm.mapped_sigungu) != ''
    GROUP BY final_gu
    ORDER BY count DESC
    LIMIT 10
    """
    
    conn = get_db_connection()
    if conn is None: return pd.DataFrame(columns=['sigungu', 'count'])
    
    try:
        df = pd.read_sql_query(sql, conn, params=params)
        df = df.rename(columns={"final_gu": "sigungu"})
        return df
    except Exception as e:
        if "no such column: dong" in str(e):
            st.error("ì˜¤ë¥˜: 'regions' í…Œì´ë¸”ì— 'dong' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. 'ë™' ë°ì´í„°ë¥¼ ë³´ì •í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        else:
            st.error(f"ì§€ì—­ë³„ ì¿¼ë¦¬ ì˜¤ë¥˜: {e}")
        return pd.DataFrame(columns=['sigungu', 'count'])
    finally:
        if conn:
            conn.close() 

# --- ë°±ì—”ë“œ í•¨ìˆ˜ 3: í”Œë«í¼ë³„ ë°ì´í„° ---
@st.cache_data
def fetch_platform_data(model, start_date, end_date):
    """í”Œë«í¼ë³„ ë§¤ë¬¼ í˜„í™© ì¡°íšŒ (ì´ í•¨ìˆ˜ëŠ” 'platform' í•„í„°ë¥¼ ë¬´ì‹œ)"""
    
    where_clause, params = build_dynamic_query_parts('ì „ì²´', model, start_date, end_date)
    sql = f"""
    SELECT pf.name, COUNT(p.post_id) as count
    FROM posts AS p
    JOIN platforms AS pf ON p.platform_id = pf.platform_id
    JOIN products AS pr ON p.product_id = pr.product_id
    {where_clause}
    GROUP BY pf.name ORDER BY count DESC
    """
    
    conn = get_db_connection()
    if conn is None: return pd.DataFrame(columns=['name', 'count'])

    try:
        df = pd.read_sql_query(sql, conn, params=params)
        return df
    except Exception as e:
        st.error(f"í”Œë«í¼ë³„ ì¿¼ë¦¬ ì˜¤ë¥˜: {e}")
        return pd.DataFrame(columns=['name', 'count'])
    finally:
        if conn:
            conn.close() 

# --- ë°±ì—”ë“œ í•¨ìˆ˜ 4: ê°€ê²© ë³€ë™ ì¶”ì´ ---
@st.cache_data
def fetch_price_trend_data(platform, model, start_date, end_date):
    """ì¼ë³„ í‰ê·  ê°€ê²© ë³€ë™ ì¶”ì´ ì¡°íšŒ"""
    
    where_clause, params = build_dynamic_query_parts(platform, model, start_date, end_date)
    
    sql = f"""
    SELECT 
        p.posted_date, 
        AVG(p.price_krw) as avg_price
    FROM posts AS p
    JOIN platforms AS pf ON p.platform_id = pf.platform_id
    JOIN products AS pr ON p.product_id = pr.product_id
    {where_clause}
    GROUP BY p.posted_date
    ORDER BY p.posted_date ASC
    """
    
    conn = get_db_connection()
    if conn is None: return pd.DataFrame(columns=['posted_date', 'avg_price'])

    try:
        df = pd.read_sql_query(sql, conn, params=params)
        return df
    except Exception as e:
        st.error(f"ê°€ê²© ì¶”ì´ ì¿¼ë¦¬ ì˜¤ë¥˜: {e}")
        return pd.DataFrame(columns=['posted_date', 'avg_price'])
    finally:
        if conn:
            conn.close() 

# --- [ì§€ë„] ì§€ë„ ìœ„ì— ì› ê·¸ë¦¬ëŠ” í•¨ìˆ˜ ---
def generate_map_overlay(region_df):
    """
    'ì„œìš¸ì§€ë„ë³´ê¸°.jpg'ë¥¼ ë¶ˆëŸ¬ì™€ì„œ region_df ë°ì´í„° ê¸°ì¤€ìœ¼ë¡œ 'ë„ë„›' ì›ì„ ê·¸ë¦½ë‹ˆë‹¤.
    """
    try:
        base_image = Image.open("ì„œìš¸ì§€ë„ë³´ê¸°.jpg").convert("RGBA") 
        draw = ImageDraw.Draw(base_image)
    except FileNotFoundError:
        return None 

    if region_df.empty:
        return base_image 

    max_count = region_df['count'].max()
    min_count = region_df['count'].min()
    
    for index, row in region_df.iloc[::-1].iterrows():
        gu_name = row['sigungu']
        count = row['count']
        
        coords = SEOUL_GU_COORDINATES.get(gu_name)
        if coords: 
            x, y = coords
            
            if max_count == min_count:
                normalized_count = 1.0
            else:
                normalized_count = (count - min_count) / (max_count - min_count)
            
            radius = 5 + (normalized_count * 15)
            alpha = int(100 + (normalized_count * 155))
            outline_width = int(1 + (normalized_count * 4))
            outline_color = (255, 87, 51, alpha) 
            
            draw.ellipse(
                (x - radius, y - radius, x + radius, y + radius),
                fill=None,
                outline=outline_color,
                width=outline_width
            )

    img_buffer = io.BytesIO()
    base_image.save(img_buffer, format='PNG')
    img_buffer.seek(0)
    
    return img_buffer

# --- UI (í”„ë¡ íŠ¸ì—”ë“œ) ---
st.title('ğŸ“± ì¤‘ê³  ì•„ì´í° ì‹œì¥ ë¶„ì„ ëŒ€ì‹œë³´ë“œ')
st.caption("í”Œë«í¼, ê¸°ì¢…, ì§€ì—­ë³„ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‹œì¥ ë™í–¥ì„ ë¶„ì„í•©ë‹ˆë‹¤.")

# --- 1. í•„í„° ì„¹ì…˜ ---
with st.container(border=True):
    col1, col2, col3, col4 = st.columns([1.3, 1.5, 2, 1.2]) 

    with col1:
        platform = st.radio(
            "**í”Œë«í¼**",
            options=['ì „ì²´', 'ë‹¹ê·¼ë§ˆì¼“', 'ì¤‘ê³ ë‚˜ë¼', 'ë²ˆê°œì¥í„°'],
            index=2, 
            horizontal=True
        )
    
    with col2:
        model = st.selectbox(
            "**ì•„ì´í° ê¸°ì¢…**",
            options=['iPhone 14', 'iPhone 14 Pro', 'iPhone 15', 'iPhone 15 Pro', 'iPhone 16', 'iPhone 16 Pro'], 
            index=5 
        )
        
    with col3:
        date_range = st.date_input(
            "**ê¸°ê°„**",
            value=(datetime.date(2025, 10, 3), datetime.date(2025, 11, 9)),
            format="YYYY-MM-DD"
        )
    
    with col4:
        st.write("") 
        analysis_button = st.button("ğŸ” ë¶„ì„ ì‹¤í–‰", type="primary", use_container_width=True)

st.divider() 

# --- 2. KPI ì„¹ì…˜ ---
kpi_container = st.container(border=True)
chart_container = st.container()

# --- 3. ë¶„ì„ ì‹¤í–‰ ë¡œì§ ---
if analysis_button:
    # ğŸ’¡ [ì¤‘ìš”] CSV íŒŒì¼ì„ ìˆ˜ì •í–ˆë‹¤ë©´, ìºì‹œë¥¼ ì§€ì›Œì•¼ í•©ë‹ˆë‹¤.
    # Streamlit í˜ì´ì§€ ìš°ì¸¡ ìƒë‹¨ ë©”ë‰´(ì  3ê°œ) > "Clear cache"ë¥¼ ëˆ„ë¥´ê±°ë‚˜, 'C' í‚¤ë¥¼ ëˆ„ë¥´ì„¸ìš”.
    
    if len(date_range) == 2:
        start_date = date_range[0]
        end_date = date_range[1]

        # 1. ëª¨ë“  ë°ì´í„° í•œ ë²ˆì— ê°€ì ¸ì˜¤ê¸°
        total_count, avg_price, id_list = fetch_kpi_and_ids(platform, model, start_date, end_date)
        
        region_df = fetch_regional_data(platform, model, start_date, end_date, MAP_FILE_PATH) 
        
        # ğŸ’¡ [ì‹ ê·œ] ë§¤í•‘ ì‹¤íŒ¨ ëª©ë¡(ë””ë²„ê·¸ìš©)ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
        unmapped_df = fetch_unmapped_dongs(platform, model, start_date, end_date, MAP_FILE_PATH)
        
        platform_df = fetch_platform_data(model, start_date, end_date) 
        price_trend_df = fetch_price_trend_data(platform, model, start_date, end_date) 
        
        map_image = generate_map_overlay(region_df)

        # 2. KPI ì¹´ë“œ ì—…ë°ì´íŠ¸
        with kpi_container:
            kpi1, kpi2, kpi3, kpi4 = st.columns(4)
            kpi1.metric(label="ì´ ë§¤ë¬¼ ìˆ˜", value=f"{total_count:,.0f} ê±´")
            kpi2.metric(label="í‰ê·  ê°€ê²©", value=f"{avg_price:,.0f} ì›")
            
            with kpi3:
                st.metric(label="ê°€ê²© ë³€ë™ (7ì¼)", value="0.0 %")
                st.caption("ê°œë°œ ì¤‘")
            
            most_frequent_region = region_df.iloc[0]['sigungu'] if not region_df.empty else "-"
            with kpi4:
                st.metric(label="ìµœë‹¤ ê±°ë˜ ì§€ì—­", value=most_frequent_region)
                if most_frequent_region != "-":
                    st.caption(f"ì´ {region_df.iloc[0]['count']}ê±´")

        # 3. ì°¨íŠ¸ ì„¹ì…˜ ì—…ë°ì´íŠ¸ (ì§€ë„ + í”Œë«í¼ + ê°€ê²© ì¶”ì´)
        with chart_container:
            chart_col1, chart_col2 = st.columns(2)
            
            with chart_col1:
                st.subheader("ğŸ“ ì§€ì—­ë³„ ë§¤ë¬¼ ë¶„í¬ (Top 10)")
                with st.container(border=True):
                    if map_image:
                        st.image(map_image, use_container_width=True)
                        
                        if not region_df.empty:
                            region_df_display = region_df.rename(columns={"sigungu": "êµ¬"}) 
                            st.dataframe(
                                region_df_display.set_index('êµ¬'), 
                                column_config={"count": "ë§¤ë¬¼ ìˆ˜"},
                                use_container_width=True
                            )
                        
                        # ğŸ’¡ [ì‹ ê·œ] ë§¤í•‘ì— ì‹¤íŒ¨í•œ 'ë™' ëª©ë¡ì„ í‘œë¡œ ë³´ì—¬ì¤ë‹ˆë‹¤ (ë””ë²„ê·¸ìš©)
                        if not unmapped_df.empty:
                            st.divider()
                            st.subheader("âš ï¸ ë§¤í•‘ ì‹¤íŒ¨í•œ 'ë™' ëª©ë¡ (Debug)")
                            st.caption("ì´ ëª©ë¡ì— 'ë™'ì´ ë³´ì¸ë‹¤ë©´, `dong_gu_map.csv` íŒŒì¼ì˜ ì´ë¦„ê³¼ DBì˜ ì´ë¦„ì´ ê³µë°± ì™¸ì— ë‹¤ë¥¸ ê¸€ì(ìˆ«ì, íŠ¹ìˆ˜ë¬¸ì ë“±)ê°€ ë‹¤ë¥¸ ê²ƒì…ë‹ˆë‹¤.")
                            st.dataframe(unmapped_df.set_index("DBì— ì €ì¥ëœ 'ë™' ì´ë¦„"), use_container_width=True)

                    else:
                        st.error("ì„œìš¸ì§€ë„ë³´ê¸°.jpg íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

            with chart_col2:
                # --- í”Œë«í¼ë³„ í˜„í™© ---
                st.subheader("ğŸ“Š í”Œë«í¼ë³„ í˜„í™©")
                with st.container(border=True): 
                    if not platform_df.empty:
                        color_map = {'ì¤‘ê³ ë‚˜ë¼': '#77DD77', 'ë²ˆê°œì¥í„°': '#FF6961', 'ë‹¹ê·¼ë§ˆì¼“': '#FFB347'}
                        df_colors = platform_df['name'].map(color_map).fillna('#D3D3D3').tolist()
                        
                        pull_values = [0] * len(platform_df)
                        line_widths = [0] * len(platform_df)
                        line_colors = ['#FFFFFF'] * len(platform_df) 
                        
                        if platform != 'ì „ì²´':
                            try:
                                platform_names = platform_df['name'].tolist()
                                if platform in platform_names:
                                    selected_index = platform_names.index(platform)
                                    pull_values[selected_index] = 0.1; line_widths[selected_index] = 4; line_colors[selected_index] = '#000000' 
                            except Exception: pass 
                        hovertemplate = "<b>%{label}</b><br>ë§¤ë¬¼ ìˆ˜: %{value:,}ê±´<br>ë¹„ìœ¨: %{percent:.1%}<extra></extra>" 
                        fig = go.Figure(data=[go.Pie(
                            labels=platform_df['name'], values=platform_df['count'], hovertemplate=hovertemplate,
                            hole=.4, pull=pull_values, textinfo='label+percent', texttemplate="%{label}<br>%{percent:.1%}", textposition='inside',
                            marker=dict(colors=df_colors, line=dict(color=line_colors, width=line_widths))
                        )])
                        fig.update_layout(margin=dict(l=0, r=0, t=0, b=0), legend=dict(orientation="h", yanchor="bottom", y=-0.1, xanchor="center", x=0.5),
                                          annotations=[dict(text='í”Œë«í¼', x=0.5, y=0.5, font_size=16, showarrow=False)])
                        st.plotly_chart(fig, use_container_width=True)
                    else:
                        st.info("í•´ë‹¹ ì¡°ê±´ì˜ í”Œë«í¼ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                
                # --- ì¼ë³„ í‰ê·  ê°€ê²© ë³€ë™ ---
                st.subheader("ğŸ“ˆ ì¼ë³„ í‰ê·  ê°€ê²© ë³€ë™") 
                with st.container(border=True):
                    if not price_trend_df.empty and len(price_trend_df) > 1:
                        fig_line = go.Figure()
                        fig_line.add_trace(go.Scatter(
                            x=price_trend_df['posted_date'], y=price_trend_df['avg_price'],
                            mode='lines+markers', name='í‰ê·  ê°€ê²©',
                            line=dict(color='#007AFF', width=2), marker=dict(size=6)
                        ))
                        fig_line.update_traces(hovertemplate="<b>%{x}</b><br>í‰ê· ê°€: %{y:,.0f}ì›<extra></extra>")
                        fig_line.update_layout(
                            yaxis_rangemode='tozero', 
                            margin=dict(l=0, r=0, t=20, b=0),
                            height=435, 
                            xaxis_title=None, yaxis_title=None,
                            hovermode="x unified"
                        )
                        st.plotly_chart(fig_line, use_container_width=True)
                        
                    elif not price_trend_df.empty and len(price_trend_df) == 1:
                        st.info("ë°ì´í„°ê°€ 1ê±´ë§Œ ìˆì–´ ì¶”ì´ë¥¼ ê·¸ë¦´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    else:
                        st.info("ê°€ê²© ë³€ë™ ì¶”ì´ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    else:
        st.warning("ë¶„ì„ ê¸°ê°„ì„ ì˜¬ë°”ë¥´ê²Œ ì„ íƒí•´ì£¼ì„¸ìš”.")

else:
    # --- í˜ì´ì§€ ì²« ë¡œë“œ ì‹œ ê¸°ë³¸ ìƒíƒœ ---
    with kpi_container:
        kpi1, kpi2, kpi3, kpi4 = st.columns(4)
        kpi1.metric(label="ì´ ë§¤ë¬¼ ìˆ˜", value="0 ê±´")
        kpi2.metric(label="í‰ê·  ê°€ê²©", value="0 ì›")
        with kpi3:
            st.metric(label="ê°€ê²© ë³€ë™ (7ì¼)", value="0.0 %")
            st.caption("ê°œë°œ ì¤‘")
        with kpi4:
            st.metric(label="ìµœë‹¤ ê±°ë˜ ì§€ì—­", value="-")
            st.caption("ê°œë°œ ì¤‘") 
            
    with chart_container:
        chart_col1, chart_col2 = st.columns(2)
        
        with chart_col1:
            st.subheader("ğŸ“ ì§€ì—­ë³„ ë§¤ë¬¼ ë¶„í¬")
            with st.container(border=True):
                try:
                    st.image("ì„œìš¸ì§€ë„ë³´ê¸°.jpg", use_container_width=True)
                except FileNotFoundError:
                    st.error("ì„œìš¸ì§€ë„ë³´ê¸°.jpg íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        with chart_col2:
            st.subheader("ğŸ“Š í”Œë«í¼ë³„ í˜„í™©")
            with st.container(border=True, height=300): 
                st.info("ë¶„ì„ ì‹¤í–‰ ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")

            st.subheader("ğŸ“ˆ ì¼ë³„ í‰ê·  ê°€ê²© ë³€ë™") 
            with st.container(border=True, height=400):
                st.info("ë¶„ì„ ì‹¤í–‰ ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")